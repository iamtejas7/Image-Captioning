{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c18d764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b1e788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebdba7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Flicker8k_text/Flickr_8k.trainImages.txt') as file:\n",
    "    photos = file.read().split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4ea58b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(photos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ab2d04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_captions(filename, photos): \n",
    "    #loading clean_captions\n",
    "    with open(filename) as f:\n",
    "        file = f.read().split('\\n')\n",
    "        \n",
    "    descriptions = {}\n",
    "    for line in file:\n",
    "        words = line.split()\n",
    "        if len(words)<1 :\n",
    "            continue\n",
    "        image, image_caption = words[0], words[1:]\n",
    "        if image in photos:\n",
    "            if image not in descriptions:\n",
    "                descriptions[image] = []\n",
    "            desc = '<start> ' + \" \".join(image_caption) + ' <end>'\n",
    "            descriptions[image].append(desc)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea408475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(photos):\n",
    "    #loading all features\n",
    "    all_features = pickle.load(open(\"features.pkl\",\"rb\"))\n",
    "    #selecting only needed features\n",
    "    features = {image:all_features[image] for image in photos}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d02a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6885df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Flicker8k_text/Flickr_8k.trainImages.txt'\n",
    "train_imgs = photos\n",
    "train_captions = load_clean_captions(\"captions.txt\", train_imgs)\n",
    "train_features = load_features(train_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c874847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f485a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_list(caption_dict):\n",
    "    all_caps = []\n",
    "    \n",
    "    for key in caption_dict.keys():\n",
    "        [all_caps.append(d) for d in caption_dict[key]]\n",
    "        \n",
    "    return all_caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "678a126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(caption_dict):\n",
    "    caps_list = dict_to_list(caption_dict)\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts(caps_list)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57b85819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7317"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = create_tokenizer(train_captions)\n",
    "pickle.dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf68d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c6d0387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = max(len(c.split()) for c in dict_to_list(train_captions))\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bd8b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc147ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(tokenizer, max_length, caps_list, feature):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    # walk through each caption for the image\n",
    "    for caps in caps_list:\n",
    "        # encode the sequence\n",
    "        seq = tokenizer.texts_to_sequences([caps])[0]\n",
    "        # split one sequence into multiple X,y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            # split into input and output pair\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # pad input sequence\n",
    "            in_seq = tf.keras.preprocessing.sequence.pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            # encode output sequence\n",
    "            out_seq = tf.keras.utils.to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            # store\n",
    "            X1.append(feature)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "            \n",
    "    return np.array(X1), np.array(X2), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7815a102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create input-output sequence pairs from the image description.\n",
    "\n",
    "#data generator, used by model.fit_generator()\n",
    "def data_generator(captions, features, tokenizer, max_length):\n",
    "    while 1:\n",
    "        for key, captions_list in captions.items():\n",
    "            #retrieve photo features\n",
    "            feature = features[key][0]\n",
    "            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, captions_list, feature)\n",
    "            yield [[input_image, input_sequence], output_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da8029e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((47, 2048), (47, 35), (47, 7317))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a,b],c = next(data_generator(train_captions, train_features, tokenizer, max_length))\n",
    "a.shape, b.shape, c.shape\n",
    "#((47, 2048), (47, 32), (47, 7577))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47823c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e5e8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c61fa459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the cnn-rnn captioning model\n",
    "def cnn_rnn_model(vocab_size, max_length):\n",
    "\n",
    "    # features from the CNN model squeezed from 2048 to 256 nodes\n",
    "    inputs1 = tf.keras.layers.Input(shape=(2048,))\n",
    "    fe1 = tf.keras.layers.Dropout(0.5)(inputs1)\n",
    "    fe2 = tf.keras.layers.Dense(256, activation='relu')(fe1)\n",
    "\n",
    "    # LSTM sequence model\n",
    "    inputs2 = tf.keras.layers.Input(shape=(max_length,))\n",
    "    se1 = tf.keras.layers.Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = tf.keras.layers.Dropout(0.5)(se1)\n",
    "    se3 = tf.keras.layers.LSTM(256)(se2)\n",
    "\n",
    "    # Merging both models\n",
    "    decoder1 = tf.keras.layers.add([fe2, se3])\n",
    "    decoder2 = tf.keras.layers.Dense(256, activation='relu')(decoder1)\n",
    "    outputs = tf.keras.layers.Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = tf.keras.models.Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    # summarize model\n",
    "    print(model.summary())\n",
    "    tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135382af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  6000\n",
      "Descriptions: train= 6000\n",
      "Photos: train= 6000\n",
      "Vocabulary Size: 7317\n",
      "Description Length:  35\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_14 (InputLayer)          [(None, 35)]         0           []                               \n",
      "                                                                                                  \n",
      " input_13 (InputLayer)          [(None, 2048)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_6 (Embedding)        (None, 35, 256)      1873152     ['input_14[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 2048)         0           ['input_13[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 35, 256)      0           ['embedding_6[0][0]']            \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 256)          524544      ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " lstm_5 (LSTM)                  (None, 256)          525312      ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 256)          0           ['dense_14[0][0]',               \n",
      "                                                                  'lstm_5[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 256)          65792       ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 7317)         1880469     ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,869,269\n",
      "Trainable params: 4,869,269\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tushar\\AppData\\Local\\Temp/ipykernel_8988/182700176.py:20: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator, epochs=1, steps_per_epoch= steps, verbose=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 3032s 504ms/step - loss: 4.4921\n",
      "6000/6000 [==============================] - 3542s 590ms/step - loss: 3.6457\n",
      "6000/6000 [==============================] - 3424s 570ms/step - loss: 3.3612\n",
      "6000/6000 [==============================] - 3861s 644ms/step - loss: 3.1936\n",
      "6000/6000 [==============================] - 3485s 581ms/step - loss: 3.0780\n",
      "6000/6000 [==============================] - 3370s 562ms/step - loss: 2.9920\n",
      "6000/6000 [==============================] - 3184s 531ms/step - loss: 2.9272\n",
      "6000/6000 [==============================] - 3255s 543ms/step - loss: 2.8700\n",
      "6000/6000 [==============================] - 3138s 523ms/step - loss: 2.8266\n",
      "6000/6000 [==============================] - 2707s 451ms/step - loss: 2.7888\n",
      "6000/6000 [==============================] - 3597s 600ms/step - loss: 2.7579\n",
      "6000/6000 [==============================] - 2777s 463ms/step - loss: 2.7320\n",
      "6000/6000 [==============================] - 3025s 504ms/step - loss: 2.7118\n",
      "6000/6000 [==============================] - 21402s 4s/step - loss: 2.6939\n",
      "6000/6000 [==============================] - 2954s 492ms/step - loss: 2.6780\n",
      "6000/6000 [==============================] - 2982s 497ms/step - loss: 2.6690\n",
      "6000/6000 [==============================] - 3160s 527ms/step - loss: 2.6505\n",
      "6000/6000 [==============================] - 3196s 533ms/step - loss: 2.6399\n",
      "6000/6000 [==============================] - 3076s 513ms/step - loss: 2.6347\n",
      "6000/6000 [==============================] - 3278s 546ms/step - loss: 2.6317\n",
      "6000/6000 [==============================] - 3510s 585ms/step - loss: 2.6206\n",
      "6000/6000 [==============================] - 3509s 585ms/step - loss: 2.6126\n",
      "6000/6000 [==============================] - 3263s 544ms/step - loss: 2.6110\n",
      "6000/6000 [==============================] - 4543s 757ms/step - loss: 2.6033\n",
      "6000/6000 [==============================] - 3282s 547ms/step - loss: 2.6018\n",
      "6000/6000 [==============================] - 3125s 521ms/step - loss: 2.5982\n",
      "6000/6000 [==============================] - 3979s 663ms/step - loss: 2.5956\n",
      "6000/6000 [==============================] - 6822s 1s/step - loss: 2.5876\n",
      "6000/6000 [==============================] - 3346s 558ms/step - loss: 2.5920\n",
      "6000/6000 [==============================] - 3301s 550ms/step - loss: 2.5884\n",
      "6000/6000 [==============================] - 3051s 508ms/step - loss: 2.5858\n",
      "6000/6000 [==============================] - 3391s 565ms/step - loss: 2.5904\n",
      "6000/6000 [==============================] - 3462s 577ms/step - loss: 2.5881\n",
      " 301/6000 [>.............................] - ETA: 44:48 - loss: 2.5737"
     ]
    }
   ],
   "source": [
    "# train our model\n",
    "print('Dataset: ', len(train_imgs))\n",
    "print('Descriptions: train=', len(train_captions))\n",
    "print('Photos: train=', len(train_features))\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Description Length: ', max_length)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model = cnn_rnn_model(vocab_size, max_length)\n",
    "\n",
    "epochs = 50\n",
    "steps = len(train_captions)\n",
    "\n",
    "# making a directory models to save our models\n",
    "os.mkdir(\"models\")\n",
    "\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(train_captions, train_features, tokenizer, max_length)\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch= steps, verbose=1)\n",
    "    model.save(\"models/model_\" + str(i) + \".h5\")\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"total time taken by model to train is : {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659a73fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
